# Transkription einer Audiodatei mithilfe des OpenAI-Modells Whisper-v2 Large sowie gpt-4o

>ü§ñ Ben√∂tigt einen eigenen OpenAI-API-Key.

>üåê Ben√∂tigt eine Internetverbindung.

>üçé Nur getestet unter macOS.

<div align="justify">
  Dieses Projekt enth√§lt drei verschiedene Python-Codes, welche zusammen mithilfe 
  eines OpenAI-API-Keys zur Transkription von Audiodateien verwendet werden k√∂nnen.
  F√ºr zus√§tzliche Informationen zur Erstellung eines pers√∂nlichen API-Keys siehe 
  OpenAI's <a href="https://platform.openai.com/docs/api-reference/authentication">API Dokumentation</a>. 
  <b>Das eigentliche Transkriptionsskript ist <i>transcription-via-OpenAI-API.py</i>, f√ºr lange oder gro√üe Audiodateien
  wird allerdings eine Vor- und Nachbereitung mithilfe von <i>audio-split.py</i> und <i>add-minutes.py</i> empfohlen.</b>
</div>

### audio-split.py
> ü•ö Schritt 1: Pre-Processing
<div align="justify">
  Das eigentliche Transkriptionsskript kann aufgrund der Limitierung bez√ºglich des Whisper-v2 Large Modells seitens OpenAI nur Audiodateien mit einer Gr√∂√üe von Maximal 25 MB
  verarbeiten. Allerdings wird empfholen, dieses Maximum nicht auszureizen und kleinere Datein zu verwenden, da das ebenfalls verwendete gpt-4o Modell ein
  begrenztes <a href="https://platform.openai.com/docs/models/gp">Token Output Limit</a> von 16 384 Tokens (Stand: November 2024) besitzt <a href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them">(<i>Was ist ein Token?</i>)</a>. Wird eine Audiodatei mit besonders vielen Worten   (also entweder aufgrund der L√§nge oder der Wortdichte der Audiodatei) 
  verwendet, kann es passieren, dass die maximale Tokengr√∂√üe √ºberschritten wird. Dies macht sich dadurch bemerkbar, dass die Transkription in der finalen Textdatei an einem Punkt 
  abbricht.</p>
  
  <p> Das <i>audio-split.py</i> Skript unterteilt eine Audiodatei in Segmente mit einer Segmentl√§nge von acht Minuten. Au√üerdem werden die Segmente in das MP3-Format mit einer
  Bitrate von 64 kbps konvertiert. Dies ist f√ºr das Whisper-v2 Large Modell vollkommen ausreichend, verringert aber die Dateigr√∂√üe. Bew√§hrt haben sich <b>Segmentl√§ngen von 8-10 Minuten</b>,
  je nach Inhalt k√∂nnten aber auch l√§ngere Segmente funktionieren, dies kann im Code selber ver√§ndert werden. </p>
</div>

### transcription-via-OpenAI-API.py
> üê£ Schritt 2: Transkription
<div align="justify">
  <p> Dies ist das eigentliche Transkriptionsskript. Zun√§chst muss innerhalb der Python-Programmierumgebung der pers√∂nliche OpenAI-API-Key als Umgebungsvariable festgelegt werden. 
      Anschlie√üend kann eine Audiodatei (oder eine Audiosegmentdatei, wie sie im ersten Schritt pr√§pariert wurde) ausgew√§hlt werden. Diese wird dann zun√§chst √ºber einen ersten API-Aufruf Wort-f√ºr-Wort 
      mithilfe des Whisper-v2 Large modells transkribiert und jedes Wort mit einem Zeitstempel versehen. Die Wort-f√ºr-Wort transkription wird dann √ºber einen zweiten API-Aufruf mithilfe des gpt-4o-Modells
      semantisch Nachbearbeitet, hierbei werden die W√∂rter kontextuell zu S√§tzen, Sinnabschnitten und Sprechersegmenten unterteilt. Die Zeitstempel bleiben f√ºr jedes Sprechersegment erhalten. Das Ergebnis ist 
      eine Textdatei mit der finalen Transkription.</p>
</div>

> ‚ö†Ô∏è Dieser Schritt f√ºhrt einen API-Call an das Whisper-v2 Large Modell sowie einen weiteren an das gpt-4o-Modell von OpenAI aus. Dementsprechend zu beachten sind die <a href="https://openai.com/api/pricing/">Kosten</a>, die sich nach der Anzahl der verarbeiteten Token richtet.

### add-minutes.py
> üê• Schritt 3: Post-Processing
<div align="justify">
  <p> Dieser Code kann verwendet werden, um die Zeitstempel eines Skripts um eine beliebige Zeit anzupassen. Die Zeit in Minuten wird dann auf jeden Zeitstempel addiert (oder subtrahiert, wenn eine negative Zahl eingegeben wird). Besonders n√ºtzlich, wenn zuvor eine l√§ngere Datei in verschiedene Segmente erstellt wurde vor dem Zusammenf√ºgen der Segmente die Zeitstempel korrigiert werden sollen.
  </p>
